name: Performance Tests

on:
  pull_request:
    branches: [main, development]
  push:
    branches: [main, development]
  release:
    types: [published, created]
  workflow_dispatch:
  schedule:
    # Run performance tests weekly on Sundays at 00:00 UTC
    - cron: '0 0 * * 0'

jobs:
  performance:
    name: Swift Performance Tests
    runs-on: macos-26
    # Performance tests are informational only - don't block PRs
    continue-on-error: true

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for comparison

      - name: Show Swift version
        run: swift --version

      - name: Find latest release tag
        id: latest_release
        run: |
          # Get the latest release tag
          LATEST_TAG=$(git describe --tags --abbrev=0 2>/dev/null || echo "")

          if [ -z "$LATEST_TAG" ]; then
            echo "No release tags found - using main branch as baseline"
            echo "baseline_ref=main" >> $GITHUB_OUTPUT
            echo "baseline_label=main branch" >> $GITHUB_OUTPUT
          else
            echo "Found latest release: $LATEST_TAG"
            echo "baseline_ref=$LATEST_TAG" >> $GITHUB_OUTPUT
            echo "baseline_label=$LATEST_TAG" >> $GITHUB_OUTPUT
          fi

      - name: Run baseline performance tests
        if: github.event_name == 'pull_request'
        id: baseline_tests
        run: |
          echo "Running baseline performance tests on ${{ steps.latest_release.outputs.baseline_label }}..."

          # Save current commit SHA (not branch, since we might be in detached HEAD)
          CURRENT_SHA=$(git rev-parse HEAD)

          # Checkout baseline ref
          git checkout ${{ steps.latest_release.outputs.baseline_ref }}

          # Create baseline output directory
          mkdir -p performance-results/baseline

          # Build and test baseline
          set +e
          swift test -c release --filter PerformanceTests > performance-results/baseline/output.txt 2>&1

          # Extract baseline metrics
          grep -E "measured\[" performance-results/baseline/output.txt > performance-results/baseline/metrics.txt || true

          # Convert to JSON
          python3 - <<'PYTHON_SCRIPT' > performance-results/baseline.json
          import json
          import re

          metrics = {}

          try:
              with open('performance-results/baseline/output.txt', 'r') as f:
                  content = f.read()

                  # Look for baseline output
                  baseline_section = re.search(
                      r'PERFORMANCE BASELINE RECORDING.*?={10,}',
                      content,
                      re.DOTALL
                  )

                  if baseline_section:
                      section_text = baseline_section.group(0)

                      for line in section_text.split('\n'):
                          match = re.search(r'([^:]+):\s+(\d+\.\d+)s', line)
                          if match:
                              name = match.group(1).strip()
                              value = float(match.group(2))
                              metrics[name] = {
                                  'name': name,
                                  'unit': 's',
                                  'value': value
                              }

              # If no baseline metrics, look for XCTest measure() results
              if not metrics:
                  with open('performance-results/baseline/metrics.txt', 'r') as f:
                      for line in f:
                          match = re.search(r'measured\[([^\]]+)\]\s+average:\s+(\d+\.\d+)', line)
                          if match:
                              name = match.group(1).strip()
                              value = float(match.group(2))
                              metrics[name] = {
                                  'name': name,
                                  'unit': 's',
                                  'value': value
                              }
          except FileNotFoundError:
              pass

          benchmarks = [
              {
                  'name': v['name'],
                  'unit': v['unit'],
                  'value': v['value']
              }
              for v in metrics.values()
          ]

          print(json.dumps(benchmarks, indent=2))
          PYTHON_SCRIPT

          echo "=== Baseline metrics ==="
          cat performance-results/baseline.json

          # Return to PR commit SHA
          git checkout $CURRENT_SHA

          exit 0

      - name: Build in Release mode
        run: |
          echo "üî® Building in Release mode"
          swift build -c release

      - name: Run performance tests
        id: perf_tests
        run: |
          echo "Running performance tests..."

          # Create output directory
          mkdir -p performance-results

          # Run performance tests in release mode and capture output
          set +e
          swift test -c release --filter PerformanceTests > performance-results/output.txt 2>&1
          PERF_EXIT_CODE=$?

          # Show test output for debugging
          echo "=== Test output (first 100 lines) ==="
          head -100 performance-results/output.txt

          echo "=== Test output (last 50 lines) ==="
          tail -50 performance-results/output.txt

          # Extract performance metrics
          echo "Extracting performance metrics..."
          grep -E "measured\[" performance-results/output.txt > performance-results/metrics.txt || true

          echo "=== Found metrics ==="
          cat performance-results/metrics.txt || echo "No XCTest metrics found"

          # Create JSON output for benchmarking
          python3 - <<'PYTHON_SCRIPT' > performance-results/benchmarks.json
          import json
          import re
          import sys

          metrics = {}

          try:
              with open('performance-results/output.txt', 'r') as f:
                  content = f.read()

                  # Look for baseline output from testRecordPerformanceBaseline
                  baseline_section = re.search(
                      r'PERFORMANCE BASELINE RECORDING.*?={10,}',
                      content,
                      re.DOTALL
                  )

                  if baseline_section:
                      section_text = baseline_section.group(0)

                      # Parse lines like "Project Creation:         0.001234s"
                      for line in section_text.split('\n'):
                          match = re.search(r'([^:]+):\s+(\d+\.\d+)s', line)
                          if match:
                              name = match.group(1).strip()
                              value = float(match.group(2))
                              metrics[name] = {
                                  'name': name,
                                  'unit': 's',
                                  'value': value
                              }

              # If no baseline metrics, look for XCTest measure() results
              if not metrics:
                  with open('performance-results/metrics.txt', 'r') as f:
                      for line in f:
                          # Parse XCTest output: "measured[Wall Clock Time, seconds] average: 0.123"
                          match = re.search(r'measured\[([^\]]+)\]\s+average:\s+(\d+\.\d+)', line)
                          if match:
                              name = match.group(1).strip()
                              value = float(match.group(2))
                              metrics[name] = {
                                  'name': name,
                                  'unit': 's',
                                  'value': value
                              }
          except FileNotFoundError:
              print("No metrics file found", file=sys.stderr)

          # Output in GitHub benchmark format
          benchmarks = [
              {
                  'name': v['name'],
                  'unit': v['unit'],
                  'value': v['value']
              }
              for v in metrics.values()
          ]

          print(json.dumps(benchmarks, indent=2))
          PYTHON_SCRIPT

          cat performance-results/benchmarks.json

          # Exit with success even if tests failed (non-blocking)
          exit 0

      - name: Format performance results for PR comment
        if: github.event_name == 'pull_request'
        id: format_results
        run: |
          echo "Formatting performance results for PR comment..."

          # Compare PR results with baseline and create markdown comment
          python3 - > performance-comment.md <<'PYTHON_SCRIPT'
          import json

          # Load PR results
          pr_results = {}
          try:
              with open('performance-results/benchmarks.json', 'r') as f:
                  pr_data = json.load(f)
                  pr_results = {item['name']: item['value'] for item in pr_data}
          except (FileNotFoundError, json.JSONDecodeError):
              pr_results = {}

          # Load baseline results
          baseline_results = {}
          baseline_label = "${{ steps.latest_release.outputs.baseline_label }}"
          try:
              with open('performance-results/baseline.json', 'r') as f:
                  baseline_data = json.load(f)
                  baseline_results = {item['name']: item['value'] for item in baseline_data}
          except (FileNotFoundError, json.JSONDecodeError):
              baseline_label = None

          # Header
          print("## üìä Performance Test Results")
          print()
          print("**SwiftProyecto** performance comparison:")
          print()

          if not pr_results:
              print("‚ö†Ô∏è No performance metrics were captured from this PR.")
              print()
              print("---")
              print("*Performance tests are informational only and do not block PRs.*")
              print()
              print("<sub>ü§ñ This comment updates automatically on new commits</sub>")
          elif not baseline_results:
              # No baseline - just show PR results
              print(f"**Baseline**: No release found - showing PR results only")
              print()
              print("| Metric | This PR |")
              print("|--------|---------|")
              for name, value in sorted(pr_results.items()):
                  print(f"| {name} | {value:.6f}s |")
              print()
              print("*Baseline comparison will be available after first release.*")
              print()
              print("---")
              print("*Performance tests are informational only and do not block PRs.*")
              print()
              print("<sub>ü§ñ This comment updates automatically on new commits</sub>")
          else:
              # Compare results
              improvements = 0
              regressions = 0
              unchanged = 0

              comparisons = []

              for name in sorted(set(pr_results.keys()) | set(baseline_results.keys())):
                  pr_value = pr_results.get(name)
                  baseline_value = baseline_results.get(name)

                  if pr_value is None:
                      comparisons.append((name, None, baseline_value, "removed", 0))
                  elif baseline_value is None:
                      comparisons.append((name, pr_value, None, "new", 0))
                  else:
                      diff_pct = ((pr_value - baseline_value) / baseline_value) * 100

                      # For time metrics, lower is better
                      if abs(diff_pct) < 1:  # Less than 1% change
                          status = "unchanged"
                          unchanged += 1
                      elif diff_pct < 0:  # PR is faster
                          status = "improved"
                          improvements += 1
                      else:  # PR is slower
                          status = "regressed"
                          regressions += 1

                      comparisons.append((name, pr_value, baseline_value, status, diff_pct))

              # Overall verdict
              if regressions == 0 and improvements > 0:
                  verdict = f"‚úÖ **Faster** - {improvements} metric(s) improved"
              elif regressions > 0 and improvements == 0:
                  verdict = f"‚ö†Ô∏è **Slower** - {regressions} metric(s) regressed"
              elif regressions > 0 and improvements > 0:
                  verdict = f"‚öñÔ∏è **Mixed** - {improvements} improved, {regressions} regressed"
              else:
                  verdict = "‚û°Ô∏è **Unchanged** - No significant performance changes"

              print(f"**Baseline**: {baseline_label}")
              print()
              print(verdict)
              print()
              print("| Metric | This PR | Baseline | Change | Status |")
              print("|--------|---------|----------|--------|--------|")

              for name, pr_val, baseline_val, status, diff_pct in comparisons:
                  if status == "new":
                      print(f"| {name} | {pr_val:.6f}s | - | - | üÜï New |")
                  elif status == "removed":
                      print(f"| {name} | - | {baseline_val:.6f}s | - | ‚ùå Removed |")
                  elif status == "unchanged":
                      print(f"| {name} | {pr_val:.6f}s | {baseline_val:.6f}s | {diff_pct:+.1f}% | ‚û°Ô∏è No change |")
                  elif status == "improved":
                      print(f"| {name} | {pr_val:.6f}s | {baseline_val:.6f}s | {diff_pct:+.1f}% | ‚úÖ Faster |")
                  else:  # regressed
                      print(f"| {name} | {pr_val:.6f}s | {baseline_val:.6f}s | {diff_pct:+.1f}% | ‚ö†Ô∏è Slower |")

              print()
              print("---")
              print("*Performance tests are informational only and do not block PRs.*")
              print()
              print("<sub>ü§ñ This comment updates automatically on new commits</sub>")
          PYTHON_SCRIPT

          # Save for next step
          cat performance-comment.md

      - name: Post PR comment
        if: github.event_name == 'pull_request'
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          header: performance-results
          path: performance-comment.md
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results-${{ github.event_name == 'release' && github.event.release.tag_name || 'dev' }}
          path: performance-results/
          retention-days: 90

      - name: Generate performance summary
        if: always()
        run: |
          echo "## Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ github.event_name }}" == "release" ]; then
            echo "**Release Performance Benchmark**: ${{ github.event.release.tag_name }}" >> $GITHUB_STEP_SUMMARY
          else
            echo "Performance tests completed. Results are informational only and do not block PRs." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f performance-results/benchmarks.json ]; then
            echo "### Metrics" >> $GITHUB_STEP_SUMMARY
            echo '```json' >> $GITHUB_STEP_SUMMARY
            cat performance-results/benchmarks.json >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "No benchmark data generated." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üìä Full results available in [artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
