name: Performance Tests

on:
  pull_request:
    branches: [main, development]
  push:
    branches: [main, development]
  release:
    types: [published, created]
  workflow_dispatch:
  schedule:
    # Run performance tests weekly on Sundays at 00:00 UTC
    - cron: '0 0 * * 0'

jobs:
  performance:
    name: iOS Performance Tests
    runs-on: macos-26
    # Performance tests are informational only - don't block PRs
    continue-on-error: true

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for comparison

      - name: Select Xcode
        run: sudo xcode-select -s /Applications/Xcode.app/Contents/Developer

      - name: Show Swift version
        run: swift --version

      - name: Find latest release tag
        id: latest_release
        run: |
          # Get the latest release tag
          LATEST_TAG=$(git describe --tags --abbrev=0 2>/dev/null || echo "")

          if [ -z "$LATEST_TAG" ]; then
            echo "No release tags found - using main branch as baseline"
            echo "baseline_ref=main" >> $GITHUB_OUTPUT
            echo "baseline_label=main branch" >> $GITHUB_OUTPUT
          else
            echo "Found latest release: $LATEST_TAG"
            echo "baseline_ref=$LATEST_TAG" >> $GITHUB_OUTPUT
            echo "baseline_label=$LATEST_TAG" >> $GITHUB_OUTPUT
          fi

      - name: Run baseline performance tests
        if: github.event_name == 'pull_request'
        id: baseline_tests
        run: |
          echo "Running baseline performance tests on ${{ steps.latest_release.outputs.baseline_label }}..."

          # Save current commit SHA (not branch, since we might be in detached HEAD)
          CURRENT_SHA=$(git rev-parse HEAD)

          # Checkout baseline ref
          git checkout ${{ steps.latest_release.outputs.baseline_ref }}

          # Clean build artifacts to ensure clean baseline build
          rm -rf ~/Library/Developer/Xcode/DerivedData/SwiftProyecto-* || true

          # Create baseline output directory
          mkdir -p performance-results/baseline

          # Build and test baseline
          set +e
          xcodebuild test \
            -scheme SwiftProyecto \
            -sdk iphonesimulator \
            -destination 'platform=iOS Simulator,name=iPhone 17 Pro' \
            -configuration Release \
            CODE_SIGNING_ALLOWED=NO \
            > performance-results/baseline/output.txt 2>&1

          # Extract baseline metrics
          grep -A 10 "üìä PERFORMANCE" performance-results/baseline/output.txt > performance-results/baseline/metrics.txt || true

          # Convert to JSON
          python3 - <<'PYTHON_SCRIPT' > performance-results/baseline.json
          import json
          import re

          metrics = {}

          try:
              with open('performance-results/baseline/metrics.txt', 'r') as f:
                  for line in f:
                      match = re.search(r'([^:]+):\s*(\d+\.\d+)s', line)
                      if match:
                          name = match.group(1).strip()
                          value = float(match.group(2))
                          metrics[name] = {
                              'name': name,
                              'unit': 'seconds',
                              'value': value
                          }
          except FileNotFoundError:
              pass

          benchmarks = [
              {
                  'name': v['name'],
                  'unit': v['unit'],
                  'value': v['value']
              }
              for v in metrics.values()
          ]

          print(json.dumps(benchmarks, indent=2))
          PYTHON_SCRIPT

          echo "=== Baseline metrics ==="
          cat performance-results/baseline.json

          # Return to PR commit SHA
          git checkout $CURRENT_SHA

          # Clean build artifacts again to ensure clean PR build
          rm -rf ~/Library/Developer/Xcode/DerivedData/SwiftProyecto-* || true

          exit 0

      - name: Build in Release mode for iOS Simulator
        run: |
          echo "üî® Building for iOS Simulator in Release mode"
          xcodebuild build \
            -scheme SwiftProyecto \
            -sdk iphonesimulator \
            -destination 'platform=iOS Simulator,name=iPhone 17 Pro' \
            -configuration Release \
            -quiet \
            CODE_SIGNING_ALLOWED=NO

      - name: Run performance tests on iOS Simulator
        id: perf_tests
        run: |
          # Run all tests to capture performance metrics
          echo "Running all tests on iOS Simulator to capture performance metrics..."

          # Create output directory
          mkdir -p performance-results

          # Run all tests in release mode and capture output
          set +e
          xcodebuild test \
            -scheme SwiftProyecto \
            -sdk iphonesimulator \
            -destination 'platform=iOS Simulator,name=iPhone 17 Pro' \
            -configuration Release \
            CODE_SIGNING_ALLOWED=NO \
            > performance-results/output.txt 2>&1
          PERF_EXIT_CODE=$?

          # Show first 100 lines of output for debugging
          echo "=== Test output (first 100 lines) ==="
          head -100 performance-results/output.txt

          # Show last 50 lines
          echo "=== Test output (last 50 lines) ==="
          tail -50 performance-results/output.txt

          # Extract performance metrics from test output
          echo "Extracting performance metrics..."
          # Get lines with "üìä PERFORMANCE" and the following 10 lines (to capture all metrics)
          grep -A 10 "üìä PERFORMANCE" performance-results/output.txt > performance-results/metrics.txt || true

          echo "=== Found metrics ==="
          cat performance-results/metrics.txt || echo "No metrics found"

          # Create JSON output for benchmarking
          python3 - <<'PYTHON_SCRIPT' > performance-results/benchmarks.json
          import json
          import re
          import sys

          metrics = {}

          try:
              with open('performance-results/metrics.txt', 'r') as f:
                  for line in f:
                      # Parse lines like "   Native load: 0.123s"
                      match = re.search(r'([^:]+):\s*(\d+\.\d+)s', line)
                      if match:
                          name = match.group(1).strip()
                          value = float(match.group(2))
                          metrics[name] = {
                              'name': name,
                              'unit': 'seconds',
                              'value': value
                          }
          except FileNotFoundError:
              print("No metrics file found", file=sys.stderr)

          # Output in GitHub benchmark format
          benchmarks = [
              {
                  'name': v['name'],
                  'unit': v['unit'],
                  'value': v['value']
              }
              for v in metrics.values()
          ]

          print(json.dumps(benchmarks, indent=2))
          PYTHON_SCRIPT

          cat performance-results/benchmarks.json

          # Exit with success even if tests failed (non-blocking)
          exit 0

      - name: Store benchmark result (development)
        uses: benchmark-action/github-action-benchmark@v1
        if: github.event_name == 'push' || github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
        with:
          name: Swift Performance Benchmarks
          tool: 'customSmallerIsBetter'
          output-file-path: performance-results/benchmarks.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          # Store results in gh-pages branch
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/bench
          # Alert if performance degrades by more than 20%
          alert-threshold: '120%'
          comment-on-alert: true
          fail-on-alert: false
          alert-comment-cc-users: '@intrusive-memory'

      - name: Store benchmark result (release)
        uses: benchmark-action/github-action-benchmark@v1
        if: github.event_name == 'release'
        with:
          name: Swift Performance Benchmarks (Release)
          tool: 'customSmallerIsBetter'
          output-file-path: performance-results/benchmarks.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          # Store release results in separate directory
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: releases/bench
          # Don't alert on releases (informational only)
          fail-on-alert: false
          comment-on-alert: false

      - name: Format performance results for PR comment
        if: github.event_name == 'pull_request'
        id: format_results
        run: |
          echo "Formatting performance results for PR comment..."

          # Compare PR results with baseline and create markdown comment
          python3 - > performance-comment.md <<'PYTHON_SCRIPT'
          import json

          # Load PR results
          pr_results = {}
          try:
              with open('performance-results/benchmarks.json', 'r') as f:
                  pr_data = json.load(f)
                  pr_results = {item['name']: item['value'] for item in pr_data}
          except (FileNotFoundError, json.JSONDecodeError):
              pr_results = {}

          # Load baseline results
          baseline_results = {}
          baseline_label = "${{ steps.latest_release.outputs.baseline_label }}"
          try:
              with open('performance-results/baseline.json', 'r') as f:
                  baseline_data = json.load(f)
                  baseline_results = {item['name']: item['value'] for item in baseline_data}
          except (FileNotFoundError, json.JSONDecodeError):
              baseline_label = None

          # Header
          print("## üìä Performance Test Results")
          print()
          print("**SwiftProyecto** performance comparison:")
          print()

          if not pr_results:
              print("‚ö†Ô∏è No performance metrics were captured from this PR.")
              print()
              print("---")
              print("*Performance tests are informational only and do not block PRs.*")
              print()
              print("<sub>ü§ñ This comment updates automatically on new commits</sub>")
          elif not baseline_results:
              # No baseline - just show PR results
              print(f"**Baseline**: No release found - showing PR results only")
              print()
              print("| Metric | This PR |")
              print("|--------|---------|")
              for name, value in sorted(pr_results.items()):
                  print(f"| {name} | {value:.4f}s |")
              print()
              print("*Baseline comparison will be available after first release.*")
              print()
              print("---")
              print("*Performance tests are informational only and do not block PRs.*")
              print()
              print("<sub>ü§ñ This comment updates automatically on new commits</sub>")
          else:
              # Compare results
              improvements = 0
              regressions = 0
              unchanged = 0

              comparisons = []

              for name in sorted(set(pr_results.keys()) | set(baseline_results.keys())):
                  pr_value = pr_results.get(name)
                  baseline_value = baseline_results.get(name)

                  if pr_value is None:
                      comparisons.append((name, None, baseline_value, "removed", 0))
                  elif baseline_value is None:
                      comparisons.append((name, pr_value, None, "new", 0))
                  else:
                      diff_pct = ((pr_value - baseline_value) / baseline_value) * 100

                      # For time metrics, lower is better
                      if abs(diff_pct) < 1:  # Less than 1% change
                          status = "unchanged"
                          unchanged += 1
                      elif diff_pct < 0:  # PR is faster
                          status = "improved"
                          improvements += 1
                      else:  # PR is slower
                          status = "regressed"
                          regressions += 1

                      comparisons.append((name, pr_value, baseline_value, status, diff_pct))

              # Overall verdict
              if regressions == 0 and improvements > 0:
                  verdict = f"‚úÖ **Faster** - {improvements} metric(s) improved"
              elif regressions > 0 and improvements == 0:
                  verdict = f"‚ö†Ô∏è **Slower** - {regressions} metric(s) regressed"
              elif regressions > 0 and improvements > 0:
                  verdict = f"‚öñÔ∏è **Mixed** - {improvements} improved, {regressions} regressed"
              else:
                  verdict = "‚û°Ô∏è **Unchanged** - No significant performance changes"

              print(f"**Baseline**: {baseline_label}")
              print()
              print(verdict)
              print()
              print("| Metric | This PR | Baseline | Change | Status |")
              print("|--------|---------|----------|--------|--------|")

              for name, pr_val, baseline_val, status, diff_pct in comparisons:
                  if status == "new":
                      print(f"| {name} | {pr_val:.4f}s | - | - | üÜï New |")
                  elif status == "removed":
                      print(f"| {name} | - | {baseline_val:.4f}s | - | ‚ùå Removed |")
                  elif status == "unchanged":
                      print(f"| {name} | {pr_val:.4f}s | {baseline_val:.4f}s | {diff_pct:+.1f}% | ‚û°Ô∏è No change |")
                  elif status == "improved":
                      print(f"| {name} | {pr_val:.4f}s | {baseline_val:.4f}s | {diff_pct:+.1f}% | ‚úÖ Faster |")
                  else:  # regressed
                      print(f"| {name} | {pr_val:.4f}s | {baseline_val:.4f}s | {diff_pct:+.1f}% | ‚ö†Ô∏è Slower |")

              print()
              print("---")
              print("*Performance tests are informational only and do not block PRs.*")
              print()
              print("<sub>ü§ñ This comment updates automatically on new commits</sub>")
          PYTHON_SCRIPT

          # Save for next step
          cat performance-comment.md

      - name: Post PR comment
        if: github.event_name == 'pull_request'
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          header: performance-results
          path: performance-comment.md
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results-${{ github.event_name == 'release' && github.event.release.tag_name || 'dev' }}
          path: performance-results/
          retention-days: 90

      - name: Generate performance summary
        if: always()
        run: |
          echo "## Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ github.event_name }}" == "release" ]; then
            echo "**Release Performance Benchmark**: ${{ github.event.release.tag_name }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "These results are stored permanently for this release version." >> $GITHUB_STEP_SUMMARY
          else
            echo "Performance tests completed. Results are informational only and do not block PRs." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f performance-results/benchmarks.json ]; then
            echo "### Metrics" >> $GITHUB_STEP_SUMMARY
            echo '```json' >> $GITHUB_STEP_SUMMARY
            cat performance-results/benchmarks.json >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "No benchmark data generated." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üìä Full results available in [artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY

          if [ "${{ github.event_name }}" == "release" ]; then
            echo "üì¶ Release benchmarks: [View Release Performance](https://intrusive-memory.github.io/SwiftProyecto/releases/bench/)" >> $GITHUB_STEP_SUMMARY
            echo "üìà Development trends: [View Dev Performance](https://intrusive-memory.github.io/SwiftProyecto/dev/bench/)" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ github.event_name }}" != "pull_request" ]; then
            echo "üìà Historical trends: [View Benchmarks](https://intrusive-memory.github.io/SwiftProyecto/dev/bench/)" >> $GITHUB_STEP_SUMMARY
          fi

  performance-report:
    name: Performance Report Summary
    runs-on: macos-26
    needs: performance
    if: always()

    steps:
      - name: Report Status
        run: |
          echo "Performance tests completed."
          echo "Status: Informational only (non-blocking)"
          echo "Results are tracked for trending analysis."
